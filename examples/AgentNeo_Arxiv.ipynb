{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa0oCgeBuM_0"
      },
      "source": [
        "# AgentNeo: Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install agentneo -U -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTC-2TkpuURw",
        "outputId": "1e4639dd-7a94-4f11-f8b7-4e7fe8b6e501"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m687.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8w5JHsBBuM_2"
      },
      "outputs": [],
      "source": [
        "from agentneo import AgentNeo, Project, Dataset, Experiment, Tracer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To login using email-id, creates new account if is not a user else retrieves the existing account\n",
        "BASE_URL = \"http://74.249.60.46:5000\"\n",
        "\n",
        "agent_session = AgentNeo(email=\"user1@example.com\", base_url=BASE_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-491WLv5sYJZ",
        "outputId": "710c4b4f-8be1-4087-bc58-acdaf94fa215"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing user found for email: user1@example.com\n",
            "Please keep your access key and secret key handy for future logins:\n",
            "Access Key: 2USyfi19AxtWqtuo3WTGww\n",
            "Secret Key: F_YZRdqZcPk5Vz7YXT4I4VGRin0e9C4T_sqOBDHiOIc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yki2wT98uM_3"
      },
      "outputs": [],
      "source": [
        "access_key = \"2USyfi19AxtWqtuo3WTGww\"\n",
        "secret_key = \"F_YZRdqZcPk5Vz7YXT4I4VGRin0e9C4T_sqOBDHiOIc\"\n",
        "\n",
        "# To login with the the access_key, secret_key\n",
        "agent_session = AgentNeo(\n",
        "        access_key=access_key,\n",
        "        secret_key=secret_key,\n",
        "        base_url=BASE_URL,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l7II2bouM_4"
      },
      "source": [
        "## Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3L3v-zYuM_5",
        "outputId": "dec28b17-dbcb-49e1-aa53-5dae893da31d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project 'Project_3' created successfully with ID: 32\n"
          ]
        }
      ],
      "source": [
        "project_created = Project(session=agent_session, project_name=\"Project_3\", description=\"A test project\").create()\n",
        "project_id = project_created['id']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWR-C8M7uM_5"
      },
      "source": [
        "## Tracing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# TODO: Save you OpenAI Key as a Secret\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "YuzlPB--0Bch"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai langchain_community langgraph arxiv -q"
      ],
      "metadata": {
        "id": "A4Ymj4Ag0IJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74197382-55ed-4075-a7f0-c0db8c893c79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMFpzUOuM_5",
        "outputId": "646f01c7-3b03-4ab8-e89c-b67ed9a0f172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the search results provided, there are indeed researchers working on the propagation of LASER through brownian fluids. \n",
            "\n",
            "1. The study titled \"Multiscale permutation entropy analysis of laser beam wandering in isotropic turbulence\" by Felipe Olivares, Luciano Zunino, Damián Gulich, Darío G. Pérez, Osvaldo A. Rosso, published in 2017, focuses on quantifying the temporal structural diversity of a laser beam propagating through isotropic optical turbulence. The study analyzes the coordinate fluctuations of the laser beam and observes a transition from an integrated stochastic process contaminated with electronic noise to a fractional Brownian motion.\n",
            "\n",
            "2. The study titled \"Galilean relativity for Brownian dynamics and energetics\" by Minghao Li, Oussama Sentissi, Stefano Azzini, Cyriaque Genet, published in 2021, experimentally studies the impact of inertial reference frame changes on overdamped Brownian motion. The study examines the consequences of Galilean transformations on Brownian diffusion and energetics, deriving a Galilean invariant expression of the stochastic thermodynamic first law.\n",
            "\n",
            "3. The study titled \"Brownian Motion Captured with a Self-Mixing Laser\" by Kenju Otsuka, Koji Kamikariya, Takayuki Ohtomo, Seiichi Sudo, Hironori Makino, published in 2008, measures the overall motion of Brownian particles suspended in water using a self-mixing thin-slice solid-state laser. The study characterizes the random walks of small particles suspended in water and identifies the overall motion as a motion of a ''virtual'' single particle with a double-peaked non-Gaussian distribution function.\n",
            "\n",
            "These studies provide insights into the behavior of LASER propagation through brownian fluids and offer valuable contributions to the field of research.\n"
          ]
        }
      ],
      "source": [
        "# Initialise the tracer by providing the tools-used and their description\n",
        "tracer = Tracer(session=agent_session, metadata={'tools': [\n",
        "    {'name': 'arxiv_tool', 'description': \"Tool to search Arxiv for available research papers.\"},\n",
        "    {'name': 'agent', 'description': \"Identify the search terms for arxiv from the user description and call the arxiv_tool based on the search term\"},\n",
        "]})\n",
        "\n",
        "# Langgraph code\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langgraph.graph import StateGraph, END\n",
        "import os\n",
        "\n",
        "openai_llm = ChatOpenAI(temperature=0.4, callbacks=[tracer.get_callback_handler()])\n",
        "arxiv = ArxivAPIWrapper(\n",
        "    top_k_results=3,\n",
        "    ARXIV_MAX_QUERY_LENGTH=300,\n",
        "    load_max_docs=3,\n",
        "    load_all_available_meta=False,\n",
        "    doc_content_chars_max=40000\n",
        ")\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[str], operator.add]\n",
        "\n",
        "@tracer.trace_node\n",
        "def agent(state):\n",
        "    query = state[\"messages\"]\n",
        "    res = openai_llm.invoke(f\"\"\"\n",
        "    You are given text summary on the research topics that the user is working on.\n",
        "    You need to extract the search term from the text summary.\n",
        "    Here is the Text summary: {query[0]}\n",
        "    \"\"\")\n",
        "    return {\"messages\": [res.content]}\n",
        "\n",
        "@tracer.trace_node\n",
        "def arxiv_tool(state):\n",
        "    context = state[\"messages\"]\n",
        "    search_query = context[1]\n",
        "    data = arxiv.run(search_query)\n",
        "    return {\"messages\": [data]}\n",
        "\n",
        "@tracer.trace_node\n",
        "def responder(state):\n",
        "    agent = openai_llm.invoke(f\"\"\"\n",
        "    You are given search results on the research topics that the user is working on.\n",
        "\n",
        "    Here is the user query:\n",
        "    ---\n",
        "    {state[\"messages\"][0]}\n",
        "    ---\n",
        "\n",
        "    Here is the search results:\n",
        "    ---\n",
        "    {state[\"messages\"][2]}\n",
        "    ---\n",
        "    \"\"\")\n",
        "    return {\"messages\": [agent.content]}\n",
        "@tracer.trace_node\n",
        "def where_to_go(state):\n",
        "    ctx = state[\"messages\"][0]\n",
        "    if ctx == \"no_response\":\n",
        "        return \"end\"\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "@tracer.trace_graph\n",
        "def workflow():\n",
        "    graph = StateGraph(AgentState)\n",
        "    graph.add_node(\"agent\", agent)\n",
        "    graph.add_node(\"arxiv\", arxiv_tool)\n",
        "    graph.add_node(\"responder\", responder)\n",
        "    graph.add_conditional_edges(\"agent\", where_to_go, {\n",
        "        \"end\": END,\n",
        "        \"continue\": \"arxiv\"\n",
        "    })\n",
        "    graph.add_edge(\"agent\", \"arxiv\")\n",
        "    graph.add_edge(\"arxiv\", \"responder\")\n",
        "    graph.set_entry_point(\"agent\")\n",
        "    graph.set_finish_point(\"responder\")\n",
        "    return graph.compile()\n",
        "\n",
        "compiled_workflow = workflow()\n",
        "\n",
        "@tracer.trace_node\n",
        "def get_output(message):\n",
        "    inputs = {\"messages\": [message]}\n",
        "    response = compiled_workflow.invoke(inputs)\n",
        "    print(response[\"messages\"][-1])\n",
        "\n",
        "get_output(\"Is someone working on the propagation of LASER through brownian fluids\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ehs6ymNuM_5",
        "outputId": "c66d327c-67f0-4f42-dd01-fd72c496e3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trace uploaded successfully. Trace ID: 35\n"
          ]
        }
      ],
      "source": [
        "trace_id = tracer.upload_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHEYhW_JuM_6"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "btGEsFjbuM_6"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from a trace\n",
        "dataset = Dataset(\n",
        "    session=agent_session,\n",
        "    project_id=project_id,\n",
        "    dataset_name=\"Dataset_1\",\n",
        "    description=\"A test dataset\"\n",
        ")\n",
        "\n",
        "dataset_traced = dataset.from_trace(trace_id=tracer.id, trace_filter=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gBisuQHuM_6"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4aTN2NJmuM_6"
      },
      "outputs": [],
      "source": [
        "# Create Experiment\n",
        "experiment = Experiment(\n",
        "        session=agent_session,\n",
        "        experiment_name=\"Experiment_1\",\n",
        "        description=\"A test experiment\",\n",
        "        dataset_id=dataset_traced['id'],\n",
        "        project_id=project_id\n",
        "    )\n",
        "\n",
        "experiment_created = experiment.create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_499m8qbuM_6"
      },
      "outputs": [],
      "source": [
        "# Execute Experiment\n",
        "exp = experiment.execute(metrics=[\n",
        "    {\"name\": \"summarise\", \"config\": {}},\n",
        "    {\"name\": \"tool_selection_accuracy\", \"config\": {\"model\": \"gpt-4o-mini\", \"OPENAI_API_KEY\": os.environ.get(\"OPENAI_API_KEY\")}},\n",
        "    {\"name\": \"tool_usage_efficiency\", \"config\": {\"model\": \"gpt-4o-mini\", \"OPENAI_API_KEY\": os.environ.get(\"OPENAI_API_KEY\")}},\n",
        "    {\"name\": \"goal_decomposition_efficiency\", \"config\": {\"model\": \"gpt-4o-mini\", \"OPENAI_API_KEY\": os.environ.get(\"OPENAI_API_KEY\")}},\n",
        "    {\"name\": \"plan_adaptibility\", \"config\": {\"model\": \"gpt-4o-mini\", \"OPENAI_API_KEY\": os.environ.get(\"OPENAI_API_KEY\")}},\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5XYT8j6uM_6",
        "outputId": "8c351f3a-6196-4e27-d7ba-f8b87216e9a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: summarise\n",
            "Result:\n",
            "completion_cost: 0.003411\n",
            "completion_tokens: 2274\n",
            "input_cost: 0.002274\n",
            "input_tokens: 4548\n",
            "latency: 0.0051250457763671875\n",
            "summary: {'children': 'list with 1 items', 'end': 'NoneType', 'inputs': 'NoneType', 'name': 'str', 'outputs': 'NoneType', 'start': 'NoneType', 'type': 'str'}\n",
            "total_cost: 0.005685\n",
            "total_tokens: 6822\n",
            "****************************************************************************************************\n",
            "\n",
            "Name: tool_selection_accuracy\n",
            "Result:\n",
            "reasoning: [\"The selected tool 'agent' is relevant as it identifies search terms for the arxiv_tool based on the user's inquiry. However, it could be argued that directly using the arxiv_tool would have been more efficient since the user is specifically looking for research papers.\", \"The selection of the 'arxiv_tool' is optimal as it directly addresses the user's request for research related to the propagation of LASER through Brownian fluids. The tool provided relevant academic papers with summaries, fulfilling the task requirements effectively.\"]\n",
            "score: 0.8749999956250001\n",
            "selected_tools: [\"agent\", \"arxiv_tool\"]\n",
            "****************************************************************************************************\n",
            "\n",
            "Name: tool_usage_efficiency\n",
            "Result:\n",
            "inefficiency_identified: The use of the 'agent' tool could be seen as redundant since it only served to generate the search term for the next tool.\n",
            "justification: The AI effectively identified the search term based on the user's inquiry, which was necessary for the subsequent tool call. However, the tool usage could have been slightly more efficient if the AI had directly called the arxiv_tool without the intermediary step, as the output was straightforward and did not require further processing.\n",
            "score: 0.85\n",
            "tools_used: ['agent']\n",
            "****************************************************************************************************\n",
            "\n",
            "Name: goal_fulfillment_rate\n",
            "Result:\n",
            "originalGoal: The user is seeking information about ongoing research related to the propagation of LASER through Brownian fluids, including identifying researchers and relevant studies.\n",
            "reason: The AI effectively decomposed the original goal into clear and logical sub-tasks that align well with the user's request. Each sub-task is appropriately assigned to the relevant tools, and the sequence of tasks follows a logical progression from identifying the search term to presenting the findings. The decomposition covers all aspects of the original goal, although the extraction and summarization could be seen as slightly overlapping with the presentation task. Overall, the decomposition is efficient and would likely scale well for similar inquiries.\n",
            "score: 0.85\n",
            "subtasks: [\"Identify the search term related to the user's inquiry.\", 'Search for relevant studies on the propagation of LASER through Brownian fluids.', 'Extract and summarize key findings from identified studies.', 'Present the titles, authors, and summaries of the relevant research.']\n",
            "****************************************************************************************************\n",
            "\n",
            "Name: plan_adaptibility\n",
            "Result:\n",
            "initial_plan: None\n",
            "justification: No plan found in the traces\n",
            "score: 0\n",
            "****************************************************************************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Experiment Results\n",
        "exp = experiment.get_results(experiment_id=exp['id'])\n",
        "\n",
        "for i in exp['results']:\n",
        "    print(f\"Name: {i['metric_name']}\")\n",
        "    print(f\"Result:\")\n",
        "    for key, value in i['result'].items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print(f\"{'*'*100}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmui7xmVuM_6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}